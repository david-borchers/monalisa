\documentclass[10pt,a4paper]{article}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[authoryear,round, longnamesfirst]{natbib}
\usepackage{textcomp}
\usepackage{setspace}
\doublespacing
\usepackage{fancyhdr}
\usepackage[]{todonotes}
\presetkeys{todonotes}{fancyline, color=white}{}

\pagestyle{fancy}
\rhead{That's not the Mona Lisa}
\lhead{}

\usepackage{lineno}
\linenumbers
%\linespread{1.6}

\renewcommand{\thesection}{S\arabic{section}}

\author[1,*]{David L. Borchers}
\author[1]{Ian Durbach}
\author[2]{Rishika Chopara}
\author[2]{Ben C. Stevenson}
\author[1]{Rachel Phillip}
\author[3]{Koustubh Sharma}

\affil[1]{Centre for Research into Ecological and Environmental Modelling, School of Mathematics and Statistics, Univeristy of St Andrews, The Observatory, St Andrews, Fife, KY16 9LZ, Scotland}
\affil[2]{Department of Statistics, University of Auckland, Auckland 1010, New Zealand}
\affil[3]{Snow Leopard Trust, Seattle, Washington, United States of America}
\affil[*]{Corresponding author: dlb@st-andrews.ac.uk}

\date{}

\title{That's not the Mona Lisa! How to interpret spatial capture-recapture density surface estimates}


\begin{document}

\maketitle

\section{Simulation study for Bayesian models}

Results presented in Section 4 were generated by fitting
maximum-likelihood SCR models to simulated data. In this appendix we
reproduce results from Section 4 using Bayesian models fitted via MCMC
to demonstrate that our results are not simply a consequence of
adopting a classical approach. In Section
\ref{sec:appendix-model-fitting} we describe our Baysian models, in
Section \ref{sec:appendix-results} we present results of our
simulation study, and in Section \ref{sec:appendix-discussion} we
discuss similarities and differences between these results and those
presented in the main manuscript based on maximum-likelihood models.

\subsection{Model fitting}
\label{sec:appendix-model-fitting}

We fitted Bayesian versions of the maximum-likelihood models presented
in Section 4 to each data set. Again, we used models with constant
density to estimate realised AC and realised usage surfaces, and a
model with inhomogeneous density characterised by a log-linear
relationship with a spatial covariate to estimate expected AC density
surfaces.

We fitted our models in NIMBLE (insert reference) using data
augmentation (REFERENCE: Tanner and Wong, 1987), which has become the
prevailing way to fit SCR models under a Bayesian framework. This
approach involves sampling a superpopulation of $M$ activity centres,
including those of the $n$ animals detected on the SCR survey. We have
an indicator variable $z_i$ for the $i$th animal, denoting whether the
$i$th animal in the augmented population is `exists' in a given MCMC
iteration. Rather than directly estimating $N$, the population size,
we estimate the data augmentation parameter, $\psi$, the expected
proportion of the animals in the superpopulation for which the
indicator is equal to 1. For each MCMC iteration we obtain a sample
from the posterior of $N$ using $\sum_{\i = 1}^M z_i$. A sample from
the posterior for animal density can be obtained by dividing by the
area of the survey region. Further details on data augmentation can be
found in Kery AND Schaub, 2012 (REFERENCE).

We used the following uninformative priors for the detection function
parameters, specifying a prior for $\log\{1/(2\sigma^2)\}$ rather than
$\sigma$ directly, as per (ROYLE TEXTBOOK):
\begin{align*}
  \lambda_0 &\sim \text{Gamma}(0.001, 0.001) \\ 
  \log\left(\frac{1}{2\sigma^2}\right) &\sim \text{Uniform}(-10, 10)
\end{align*}

For the constant density model, the activity centres were given a
uniform prior distribution over the survey region and the data
augmentation parameter was given a uniform prior from 0 to 1. For the
inhomogeneous density model, animal density at location $\bm{x}$ is
given by $D(\bm{x}) = \exp\{\beta_0 + \beta_1 y(\bm{x})\}$, where
$y(\bm{x})$ is a measurement of a covariate at location $\bm{x}$. We
used the following uninformative priors for the coefficients $\beta_0$
and $\beta_1$:
\begin{align*}
\beta_0 &\sim \text{Uniform}(-10, 10) \\
\beta_1 &\sim \text{Uniform}(-10, 10)
\end{align*}

When we fit each constant density model, we ran () MCMC iterations,
where we set M to be equal to (). We also used a thinning value of (),
an adapatation interval of () and () burn-in iterations.

When fitting each inhomogeneous density model, we ran () MCMC iterations,
and used a value of () for M. The thinning value was (), with an adaptation
interval of () and () burn-in iterations.

\subsection{Results}
\label{sec:appendix-results}

<<fig9, echo = FALSE, include = TRUE, fig.cap = "Figure 9", message=FALSE, fig.width=10, warning=FALSE>>=

## ---------------------------------------------------------------------------------------
# First, creating the necessary data objects
## ---------------------------------------------------------------------------------------

# Sourcing in 'posthoc_extract_chs.R' to extract the necessary capture histories
load("../../output/mona_raw_outputs_100sim.RData")
source("../../code/posthoc_extract_chs.R")

## 3 sampling occasions (first column)
# Summing the capture histories over the 3 sampling occasions
encounterdat.3occ = matrix(0, nrow=nrow(ch8a[[1]][,1,]), ncol=ncol(ch8a[[1]][,1,]))
for (i in 1:3) {
  encounterdat.3occ = encounterdat.3occ + ch8a[[1]][,i,]
}
# Trap locations
trap.loc = attributes(ch8a[[1]])$traps
# xlim, ylim (we know these)
xlim = c(0.5, 50.5)
ylim = c(0.5, 50.5)
# Creating the data object we will use later
data.3occ = list(encounter.data = encounterdat.3occ, trap.loc = trap.loc, xlim = xlim, ylim = ylim, n.occasions = 3)

## 10 sampling occasions (second column)
# Summing the capture histories over all 10 sampling occasions
encounterdat.10occ = matrix(0, nrow=nrow(ch8b[[1]][,1,]), ncol=ncol(ch8b[[1]][,1,]))
for (i in 1:10) {
  encounterdat.10occ = encounterdat.10occ + ch8b[[1]][,i,]
}
# Creating the data object (uses same trap locs, xlim, ylim as above)
data.10occ = list(encounter.data = encounterdat.10occ, trap.loc = trap.loc, xlim = xlim, ylim = ylim, n.occasions = 10)

## 20 sampling occasions (third column)
# Summing the capture histories over all 20 sampling occasions
encounterdat.20occ = matrix(0, nrow=nrow(ch8c[[1]][,1,]), ncol=ncol(ch8c[[1]][,1,]))
for (i in 1:20) {
  encounterdat.20occ = encounterdat.20occ + ch8c[[1]][,i,]
}
# Creating the data object
data.20occ = list(encounter.data = encounterdat.20occ, trap.loc = trap.loc, xlim = xlim, ylim = ylim, n.occasions = 20)

## ---------------------------------------------------------------------------------------
# Loading in the MCMC objects
## ---------------------------------------------------------------------------------------

load("../../bayesian_code/Fig9_MCMC_3occ.RData")
load("../../bayesian_code/Fig9_MCMC_10occ.RData")
load("../../bayesian_code/Fig9_MCMC_20occ.RData")

## ---------------------------------------------------------------------------------------
# Creating the objects we need
## ---------------------------------------------------------------------------------------

## Row 1: RACD maps. Are creating vectors that contain the density values for each pixel
source("../../bayesian_code/DensityVectorFunction_RACDMaps.R")

density.3occ.all = no.movement.density.vector(results=results.3occ, M=300, xlim=c(0.5, 50.5), ylim=c(0.5, 50.5))
density.10occ.all = no.movement.density.vector(results=results.10occ, M=300, xlim=c(0.5, 50.5), ylim=c(0.5, 50.5))
density.20occ.all = no.movement.density.vector(results=results.20occ, M=300, xlim=c(0.5, 50.5), ylim=c(0.5, 50.5))

## Row 2: RUD maps. We will load in the vectors that contain the density values for each pixel (otherwise, code will take a while to run)
load("../../bayesian_code/Fig9_RUD_3occ.RData")
load("../../bayesian_code/Fig9_RUD_10occ.RData")
load("../../bayesian_code/Fig9_RUD_20occ.RData")

## Matrix of pixel centres
source("../../bayesian_code/RUDMaps_Functions.R")
pixel.centres = centres(xrange=c(0.5,50.5), yrange=c(0.5,50.5), x.pixels=50, y.pixels=50)

## ---------------------------------------------------------------------------------------
# Code to generate the plot
## ---------------------------------------------------------------------------------------

# Libraries needed
library(tidyverse)
library(viridis)
library(patchwork)
library(scales)

# Loading RData object that we need
load("../../output/capthist_summaries_100sim.RData")

# process the outputs
detectors_df_all <- fig67_results_100sim %>% purrr::map_depth(2, "detectors_df") %>% map_df(bind_rows)
detectors_df_all <- detectors_df_all %>% distinct()


## Creating objects that contain the density values for each map, along with other required information
# 3 sampling occasions
df.3occ.all <- data.frame(pixel.centres, density.3occ.all, as.factor(rep(3,2500)),
                         as.factor(rep("None", 2500)))
names(df.3occ.all) <- c("x", "y", "value", "occasions", "movetype")
# 10 sampling occasions
df.10occ.all <- data.frame(pixel.centres, density.10occ.all, as.factor(rep(10,2500)),
                          as.factor(rep("None", 2500)))
names(df.10occ.all) <- c("x", "y", "value", "occasions", "movetype")
# 20 sampling occasions
df.20occ.all <- data.frame(pixel.centres, density.20occ.all, as.factor(rep(20,2500)),
                          as.factor(rep("None", 2500)))
names(df.20occ.all) <- c("x", "y", "value", "occasions", "movetype")
# Combining these objects into one data frame
ac_densities_without_movement <- rbind.data.frame(df.3occ.all, df.10occ.all, df.20occ.all)

# 3 sampling occasions
df.3occ.all.withmov <- data.frame(pixel.centres, density.3occ.all.withmov, as.factor(rep(3,2500)),
                                 as.factor(rep("With movement", 2500)))
names(df.3occ.all.withmov) <- c("x", "y", "value", "occasions", "movetype")
# 10 sampling occasions
df.10occ.all.withmov <- data.frame(pixel.centres, density.10occ.all.withmov, as.factor(rep(10,2500)),
                                  as.factor(rep("With movement", 2500)))
names(df.10occ.all.withmov) <- c("x", "y", "value", "occasions", "movetype")
# 20 sampling occasions
df.20occ.all.withmov <- data.frame(pixel.centres, density.20occ.all.withmov, as.factor(rep(20,2500)),
                                  as.factor(rep("With movement", 2500)))
names(df.20occ.all.withmov) <- c("x", "y", "value", "occasions", "movetype")
# Combining these objects into one data frame
ac_densities_with_movement <- rbind.data.frame(df.3occ.all.withmov, df.10occ.all.withmov, df.20occ.all.withmov)

# Combining both data frames now
ac_densities_with_movement = rbind.data.frame(ac_densities_without_movement, ac_densities_with_movement)


# detectors are the same for all plots so just extract unique combos of (x,y)
detectors <- detectors_df_all %>% group_by(x,y) %>% count()

# Column labels for plots
capthist_labels = paste(c(sum(encounterdat.3occ), sum(encounterdat.10occ), sum(encounterdat.20occ)), "detections\n", paste("(", c(nrow(encounterdat.3occ), nrow(encounterdat.10occ), nrow(encounterdat.20occ)), sep=""),  "individuals)")

# relabel factor levels for occasion variable
ac_densities_with_movement$occasions <- factor(ac_densities_with_movement$occasions,
                                            levels = c(3,10,20),
                                            labels = capthist_labels)

# scale the plots to have min 0 and max 1
# (need to think about best way to scale things for visualisation)
ac_densities_with_movement2 <- ac_densities_with_movement %>%
  group_by(occasions, movetype) %>%
  mutate(value = (value - min(value)) / (max(value) - min(value))) %>%
  ungroup()

p2a <- ac_densities_with_movement %>%
  filter(movetype == "None") %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = value)) +
  #scale_fill_gradientn(colours = pal, limits = c(0,0.3), breaks = c(0,0.1,0.2,0.3)) +
  scale_fill_viridis(direction = 1, option = "viridis", limits = c(0,0.3), breaks = c(0,0.1,0.2,0.3)) +
  facet_grid(movetype ~ occasions) +
  geom_point(data = detectors, aes(x,y),
             colour = "black", pch = 4, size = 2) +
  theme(axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position="right", legend.key.height = unit(0.7,"cm"), legend.title = element_blank(),
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

p2b <- ac_densities_with_movement %>%
  filter(movetype == "With movement") %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = value)) +
  #scale_fill_gradientn(colours = pal, limits = c(0,0.1), breaks = c(0,0.05,0.1)) +
  #scale_fill_viridis(direction = 1, option = "viridis", trans = "log10", limits = c(1,103), oob = squish) +
  scale_fill_viridis(direction = 1, option = "viridis", trans = "log10") +
  facet_grid(movetype ~ occasions) +
  geom_point(data = detectors, aes(x,y),
             colour = "black", pch = 4, size = 2) +
  theme(strip.text.x = element_blank(),
        axis.line=element_blank(),axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        legend.position="right", legend.key.height = unit(0.7,"cm"), legend.title = element_blank(),
        panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank())

p2 <- p2a / p2b

p2
@ 

\subsection{Discussion}
\label{sec:appendix-discussion}

\section{Estimation of realised usage density}

Estimation of realised usage density is a similar process for both
maximum likelihood and Bayesian approaches: we sum usage densities for
each individual animal, each of which is calculated by convolving the
estimated PDF of its activity centre with an individual usage
distribution.

\subsection{The maximum likelihood approach}

For maximum likelihood, the estimated usage density for the $i$th
animal, with capture history $\bm{\omega}_i$, is given by
\begin{equation}
f_{\bm{s} \mid \bm{\omega}}(\bm{s} \mid \bm{\omega}_i; \bm{\widehat{\theta}}) =
\int f_{\bm{x} \mid \bm{\omega}}(\bm{x} \mid \bm{\omega}_i; \bm{\widehat{\theta}})
f_{\bm{s} \mid \bm{x}}(\bm{s} \mid \bm{x}; \bm{\widehat{\theta}}) d\bm{x}, \label{eq:ind-usage}
\end{equation}
where
\begin{itemize}
\item $\bm{\widehat{\theta}}$ is a vector containing the maximum
  likelihood estimates of the encounter function parameters;
\item $f_{\bm{s} \mid \bm{\omega}}(\bm{s} \mid
  \bm{\omega}_i; \bm{\widehat{\theta}})$ is the estimated usage distribution, providing the
  probability density of finding an individual with capture history
  $\bm{\omega}_i$ at location $\bm{s}$ at a randomly selected point in
  time;
\item $f_{\bm{x} \mid \bm{\omega}}(\bm{x} \mid \bm{\omega}_i;
  \bm{\widehat{\theta}})$ is the estimated PDF of the activity centre
  of an individual with capture history $\bm{\omega}_i$ (see Section
  3); and
\item $f_{\bm{s} \mid \bm{x}}(\bm{s} \mid \bm{x}; \bm{\widehat{\theta}})$ is the
  estimated usage distribution of the individual conditional on the
  activity centre, providing the probability density of the individual
  being at location $\bm{s}$ given that its activity centre is at
  $\bm{x}$.
\end{itemize}
Estimated usage density at location $\bm{s}$ is then given by
$\widehat{D}_u(\bm{s}) = \sum_i f_{\bm{s} \mid
  \bm{\omega}}(\bm{s} \mid \bm{\omega}_i; \bm{\widehat{\theta}})$, noting that the sum is
over individuals that were not detected, with capture histories $(0,
\cdots, 0)$, along with those that were.

\todo[inline]{It's unclear to me whether we directly estimate a
  detection function or an encounter function in our models. Below I
  assume the reader will know what an encounter fucntion is, but it
  might need to be explained more explicitly. It's important that we
  construct the individual usage distribution using an encounter
  function rather than a detection function, because the rate at which
  an animal visits a location is proportional to the encounter
  function, but not to the detection function.} Here we constructed
the individual usage distribution under the assumption that the
density of an individual being at location $\bm{s}$ given its activity
centre is at $\bm{x}$ is proportional to the encounter function
$h\{d(\bm{s}, \bm{x}); \widehat{\bm{\theta}}\}$, where $d(\bm{s},
\bm{x})$ is the Euclidean distance between $\bm{s}$ and $\bm{x}$, and
so
\begin{equation}
  f_{\bm{s} \mid \bm{x}}(\bm{s} \mid \bm{x}; \widehat{\bm{\theta}}) = \frac{h\{d(\bm{s}, \bm{x}); \widehat{\bm{\theta}}\}}{\int h\{d(\bm{s}^\prime, \bm{x}); \widehat{\bm{\theta}}\} d\bm{s}^\prime},
\end{equation}
where the denominator is a normalising constant.
\subsection{The Bayesian approach}

Bayesian models fitted via MCMC can directly sample activity centres
of detected individuals, and also of undetected individuals using data
augmentation (ROYLE TEXTBOOK REFERENCE), thus obtaining samples from
$f_{\bm{x} \mid \bm{\omega}}(\bm{x} \mid \bm{\omega})$ for each
individual. We can use these samples directly to obtain the following
approximation of the $i$th individual's usage distribution:
\begin{equation}
  f_{\bm{s} \mid \bm{\omega}}(\bm{s} \mid \bm{\omega}_i) \approx
  \frac{1}{M} \sum_{j = 1}^M f_{\bm{s} \mid \bm{x}}(\bm{s} \mid
  \bm{x}_{(j)}, \bm{\theta}_{(j)}),
\end{equation}
where $\bm{x}_{(j)}$ and $\bm{{\theta}}_{(j)}$ are the activity centre
and a vector of encounter function parameters that were sampled on the
$j$th of $M$\todo{Err, we used $M$ earlier for superpopulation
  size. What should this be?} total MCMC iterations, respectively. The
estimated usage distribution is therefore not conditional on one
particular set of estimated parameter values, but instead considers
the range of values across the posterior distribution of
$\bm{\theta}$.

\subsection{Discussion}

\todo[inline]{I'm not sure that this is the best place for the
  discussion below, but leaving it here for now.}

We constructed individual usage distributions using the encounter
function from our SCR model, but this may not always be
appropriate. For example, if individuals cannot fully explore their
home range within the duration of the survey, then we would not expect
the spatial range of the detection function to match the extent of an
animal's usage distribution.

Even for longer surveys, it may not be sensible to relate the range of
the encounter function to the size of the region used by an individual
even for longer surveys, so care should be taken when this practice is
used. For example, \citet{Tenan+al:17} found that the spatial scale of
the encounter rate function for brown bears (\emph{Ursus arctos})
estimated using SCR was not consistent with spatial usage parameters
estimated from other data sources, although \citet{Popescu+al:14} did
not detect any such inconsistency for a population of fishers
(\emph{Pekania pennanti}). If alternative data sources are available
(e.g., telemetry, or opportunistic data such as hair or scat samples)
they may be incoprorated for improved estimation of individual usage
distributions \citep{Tenan+al:17}.

Our method also assumes that home ranges are circular, however their
shapes are likely to be modified by variables relating to population
and landscape connectivity \citep[see][for a review]{Drake+al:ip}.


\bibliographystyle{../mee} \bibliography{../monalisa}

\end{document}
